[[lab2]]
= Lab 2: {lab2-title}

By doing this lab, my intention was to discover if it was easy to create an infrastructure prepared to run SCDF in {pivotal-cloud-foundry}.
Also, one of my goals was to switch from {Kafka} to {RabbitMQ}.

== Prerequisites

. {VirtualBox} installed (PCF Dev uses it).
. An account on {uri-pivotal-cloud-foundry}[Pivotal Web Services] (do download PCF Dev binary).

== Step 1 - Download and install PCF Dev

First of all, I https://network.pivotal.io/products/pcfdev[downloaded] and https://pivotal.io/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/install-pcf-dev[install PCF Dev CLI].
I typed the following commands:

----
$ unzip pcfdev-v0.30.0+PCF1.11.0-osx.zip
Archive:  pcfdev-v0.30.0+PCF1.11.0-osx.zip
  inflating: pcfdev-v0.30.0+PCF1.11.0-osx

$ ./pcfdev-v0.30.0+PCF1.11.0-osx
Plugin successfully upgraded. Current version: 0.30.0. For more info run: cf dev help

$ cf dev version
PCF Dev version 0.30.0 (CLI: 850ae45, OVA: 0.549.0)
----

== Step 2 - Start PCF Dev

After installed, I started PCF Dev with more memory than default (4096 MB makes the services very hard to work):

----
$ cf dev start -m 8192
Downloading VM...
Progress: |====================>| 100%
VM downloaded.
Allocating 8192 MB out of 16384 MB total system memory (9217 MB free).
Importing VM...
Starting VM...
Provisioning VM...
Waiting for services to start...
7 out of 58 running
7 out of 58 running
7 out of 58 running
7 out of 58 running
40 out of 58 running
56 out of 58 running
58 out of 58 running
 _______  _______  _______    ______   _______  __   __
|       ||       ||       |  |      | |       ||  | |  |
|    _  ||       ||    ___|  |  _    ||    ___||  |_|  |
|   |_| ||       ||   |___   | | |   ||   |___ |       |
|    ___||      _||    ___|  | |_|   ||    ___||       |
|   |    |     |_ |   |      |       ||   |___  |     |
|___|    |_______||___|      |______| |_______|  |___|
is now running.
To begin using PCF Dev, please run:
   cf login -a https://api.local.pcfdev.io --skip-ssl-validation
Apps Manager URL: https://apps.local.pcfdev.io
Admin user => Email: admin / Password: admin
Regular user => Email: user / Password: pass
----

[NOTE]
====
The command above takes some time (around 15 minutes on my MacBook Pro) because it has to download the VM (in https://en.wikipedia.org/wiki/Open_Virtualization_Format[OVA format]) and starts a bunch of services (the most time-consuming part of the procedure).
After it finishes I verified the directory structure where the machine was installed:

----
$ tree ~/.pcfdev/
/Users/pj/.pcfdev/
|-- ova
|   `-- pcfdev-v0.549.0.ova
|-- token
`-- vms
    |-- key.pem
    |-- pcfdev-v0.549.0
    |   |-- Logs
    |   |   |-- VBox.log
    |   |   `-- VBox.log.1
    |   |-- pcfdev-v0.549.0-disk1.vmdk
    |   |-- pcfdev-v0.549.0.vbox
    |   `-- pcfdev-v0.549.0.vbox-prev
    `-- vm_config

4 directories, 9 files
----
====

== Step 3 - Login into PCF Dev

When PCF Dev was started, I made my login:

----
$ cf login -a https://api.local.pcfdev.io --skip-ssl-validation -u admin -p admin -o pcfdev-org
API endpoint: https://api.local.pcfdev.io
Authenticating...
OK

Targeted org pcfdev-org

Targeted space pcfdev-space



API endpoint:   https://api.local.pcfdev.io (API version: 2.82.0)
User:           admin
Org:            pcfdev-org
Space:          pcfdev-space
----

== Step 4 - Create services

After that, I created three services:

{MySQL} service:

----
$ cf create-service p-mysql 512mb df-mysql
Creating service instance df-mysql in org pcfdev-org / space pcfdev-space as admin...
OK
----

{RabbitMQ} service:

----
$ cf create-service p-rabbitmq standard df-rabbitmq
Creating service instance df-rabbitmq in org pcfdev-org / space pcfdev-space as admin...
OK
----

{Redis} service:

----
$ cf create-service p-redis shared-vm df-redis
Creating service instance df-redis in org pcfdev-org / space pcfdev-space as admin...
OK
----

== Step 5 - Download some jars

----
wget -c http://repo.spring.io/release/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/1.4.0.RELEASE/spring-cloud-dataflow-server-cloudfoundry-1.4.0.RELEASE.jar
----

== Step 6 - Create a manifest.yml file

----
cat > manifest.yml <<'EOF'
---
applications:
- name: dataflow-server
  memory: 1g
  disk_quota: 2g
  path: spring-cloud-dataflow-server-cloudfoundry-1.4.0.RELEASE.jar
  buildpack: java_buildpack
  services:
    - df-mysql
    - df-redis
  env:
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: df-rabbitmq
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-snapshot
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_MEMORY: 512
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_DISK: 512
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_INSTANCES: 1
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_BUILDPACK: java_buildpack
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX: false
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: df-mysql
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_MEMORY: 512
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_DISK: 512
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_INSTANCES: 1
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_BUILDPACK: java_buildpack
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_ENABLE_RANDOM_APP_NAME_PREFIX: false
    SPRING_CLOUD_DATAFLOW_FEATURES_EXPERIMENTAL_TASKSENABLED: true
EOF
----

See the session "Deploying using a Manifest" of the document {uri-spring-cloud-dataflow-server-cloudfoundry-doc}["Spring Cloud Data Flow Server for Cloud Foundry"] for more information.

== Step 7 - Push spring-cloud-dataflow-server-cloudfoundry to PCF Dev

----
$ cf push
Pushing from manifest to org pcfdev-org / space pcfdev-space as admin...
Using manifest file /Users/pj/labs/spring-cloud-dataflow-tutorial/manifest.yml
Getting app info...
Creating app with these attributes...
+ name:         dataflow-server
  path:         /Users/pj/labs/spring-cloud-dataflow-tutorial/spring-cloud-dataflow-server-cloudfoundry-1.4.0.RELEASE.jar
+ buildpack:    java_buildpack
+ disk quota:   2G
+ memory:       1G
  services:
+   df-mysql
+   df-redis
  env:
+   MAVEN_REMOTE_REPOSITORIES_REPO1_URL
+   SPRING_CLOUD_DATAFLOW_FEATURES_EXPERIMENTAL_TASKSENABLED
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_BUILDPACK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_DISK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_INSTANCES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_MEMORY
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_BUILDPACK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_DISK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_ENABLE_RANDOM_APP_NAME_PREFIX
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_INSTANCES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_MEMORY
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME
  routes:
+   dataflow-server.local.pcfdev.io

Creating app dataflow-server...
Mapping routes...
Binding services...
Comparing local files to remote cache...
Packaging files to upload...
Uploading files...
 71.59 MiB / 71.59 MiB [==================================================================================] 100.00% 1s

Waiting for API to complete processing files...

Staging app and tracing logs...
   Downloaded java_buildpack (244.5M)
   Creating container
   Successfully created container
   Downloading app package...
   Downloaded app package (71.6M)
   Staging...
   -----> Java Buildpack Version: v3.13 (offline) | https://github.com/cloudfoundry/java-buildpack.git#03b493f
   -----> Downloading Open Jdk JRE 1.8.0_121 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_121.tar.gz (found in cache)
          Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.0s)
   -----> Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (found in cache)
          Memory Settings: -Xss349K -Xms681574K -XX:MetaspaceSize=104857K -Xmx681574K -XX:MaxMetaspaceSize=104857K
   -----> Downloading Container Certificate Trust Store 2.0.0_RELEASE from https://java-buildpack.cloudfoundry.org/container-certificate-trust-store/container-certificate-trust-store-2.0.0_RELEASE.jar (found in cache)
          Adding certificates to .java-buildpack/container_certificate_trust_store/truststore.jks (0.3s)
   -----> Downloading Spring Auto Reconfiguration 1.10.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.10.0_RELEASE.jar (found in cache)
   Exit status 0
   Staging complete
   Uploading droplet, build artifacts cache...
   Uploading build artifacts cache...
   Uploading droplet...
   Uploaded build artifacts cache (109B)
   Uploaded droplet (116.9M)
   Uploading complete
   Destroying container
   Successfully destroyed container

Waiting for app to start...

name:              dataflow-server
requested state:   started
instances:         1/1
usage:             1G x 1 instances
routes:            dataflow-server.local.pcfdev.io
last uploaded:     Sun 29 Apr 13:18:54 WEST 2018
stack:             cflinuxfs2
buildpack:         java_buildpack
start command:     CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE
                   -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10
                   -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) &&
                   JAVA_OPTS="-Djava.io.tmpdir=$TMPDIR
                   -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY
                   -Djavax.net.ssl.trustStore=$PWD/.java-buildpack/container_certificate_trust_store/truststore.jks
                   -Djavax.net.ssl.trustStorePassword=java-buildpack-trust-store-password" && SERVER_PORT=$PORT eval
                   exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/.
                   org.springframework.boot.loader.JarLauncher

     state     since                  cpu      memory         disk         details
#0   running   2018-04-29T12:19:56Z   246.0%   476.3M of 1G   206M of 2G   
----

== Step 8 - Access the dashboard

After deployed, I pointed my browser to to following URL: http://dataflow-server.local.pcfdev.io/dashboard/.

== Step 9 - Use the shell

In my <<lab1,previous lab>>, I used the UI interface (through the browser) to create a stream.
This time, however, I created the stream by using the command line.
These were my steps:

----
$ wget -c http://repo.spring.io/release/org/springframework/cloud/spring-cloud-dataflow-shell/1.4.0.RELEASE/spring-cloud-dataflow-shell-1.4.0.RELEASE.jar
----

----
$ java -jar spring-cloud-dataflow-shell-1.4.0.RELEASE.jar 
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

1.4.0.RELEASE

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:>
----

----
server-unknown:>dataflow config server http://dataflow-server.local.pcfdev.io
Shell mode: classic, Server mode: classic
dataflow:>
----

----
dataflow:>app import --uri http://bit.ly/Celsius-SR1-stream-applications-rabbit-maven
Successfully registered applications: [sink.task-launcher-yarn, source.tcp, sink.jdbc, source.http, sink.rabbit, source.rabbit, source.ftp, sink.gpfdist, processor.transform, source.loggregator, source.sftp, processor.filter, source.file, sink.cassandra, processor.groovy-filter, sink.router, source.trigger, sink.hdfs-dataset, processor.splitter, source.load-generator, sink.sftp, sink.file, processor.tcp-client, source.time, source.gemfire, source.twitterstream, sink.tcp, source.jdbc, sink.field-value-counter, sink.redis-pubsub, sink.hdfs, sink.task-launcher-local, processor.bridge, processor.pmml, processor.httpclient, sink.ftp, source.s3, sink.log, sink.gemfire, sink.aggregate-counter, sink.throughput, source.triggertask, sink.s3, source.gemfire-cq, source.jms, source.tcp-client, processor.scriptable-transform, sink.counter, sink.websocket, source.mongodb, source.mail, processor.groovy-transform, source.syslog]
----

----
dataflow:>stream create --name httptest --definition "http | log" --deploy
Created new stream 'httptest'
Deployment request has been sent
----

----
dataflow:>stream list
╔═══════════╤═════════════════╤═════════════════════════════════════════╗
║Stream Name│Stream Definition│                 Status                  ║
╠═══════════╪═════════════════╪═════════════════════════════════════════╣
║httptest   │http | log       │The stream has been successfully deployed║
╚═══════════╧═════════════════╧═════════════════════════════════════════╝
----

I started another shell (leaving the current shell opened) and then I typed the following command:

----
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                            requested state   instances   memory   disk   urls
dataflow-server                 started           1/1         1G       2G     dataflow-server.local.pcfdev.io
dataflow-server-httptest-http   started           1/1         512M     512M   dataflow-server-httptest-http.local.pcfdev.io
dataflow-server-httptest-log    started           1/1         512M     512M   dataflow-server-httptest-log.local.pcfdev.io
----

In order to view the log output for `dataflow-server-httptest-log`, I typed:

-----
$ cf logs dataflow-server-httptest-log
Retrieving logs for app dataflow-httptest-log in org pcfdev-org / space pcfdev-space as admin...

-----

Back to the first shell (running `spring-cloud-dataflow-shell`), I typed:

----
dataflow:>http post --target http://dataflow-server-httptest-http.local.pcfdev.io --data "hello world"
> POST (text/plain;Charset=UTF-8) http://dataflow-httptest-http.local.pcfdev.io hello world
> 202 ACCEPTED

dataflow:>http post --target http://dataflow-server-httptest-http.local.pcfdev.io --data "paulo jeronimo"
> POST (text/plain) http://dataflow-server-httptest-http.local.pcfdev.io paulo jeronimo
> 202 ACCEPTED
----

So, this lines appears in the output of the command `cf logs dataflow-server-httptest-log`:

----
   2018-04-29T14:15:04.21+0100 [APP/PROC/WEB/0] OUT 2018-04-29 13:15:04.215  INFO 6 --- [http.httptest-1] log.sink                                 : hello world
   2018-04-29T14:17:22.83+0100 [APP/PROC/WEB/0] OUT 2018-04-29 13:17:22.832  INFO 6 --- [http.httptest-1] log.sink                                 : paulo jeronimo
----

My last command was delete the created stream:

----
dataflow:>stream destroy --name httptest
Destroyed stream 'httptest'
----

== Step 10 - Stop PCF Dev

----
cf dev stop
----

[[lab2-conclusions]]
== Conclusions

Positive points: ::
* The PCF Dev also makes the deployment of SCDF components very simple.
The `cf` command makes easy, in a development environment, to push new sources, sinks, or processors to the servers.
* The shell interface provided by <<spring-cloud-dataflow-shell>> is extremely useful to automate things.
This kind of tool is totally required in a DevOps world.
* Based on this lab, I can deduce that in a large corporation with {uri-pivotal-cloud-foundry}[Pivotal Cloud Foundry] installed on its servers, SCDF is definitely prepared and integrated with this kind of infrastructure.
Negative points: ::
* The time to start PCF Dev on my machine was very slow compared to the time to up Docker containers with Docker compose.
So, I would recommend the use of Docker in a developer environment instead of PCF Dev.

== References

* {oA}
* {uri-spring-cloud-dataflow-samples-docs}[Spring Cloud Data Flow Samples Documentation]
