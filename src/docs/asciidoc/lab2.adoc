[[lab2]]
= Lab 2: {lab2-title}

By doing this lab, my intention was to discover if it was easy to create an infrastructure prepared to run SCDF in {pivotal-cloud-foundry}.
Also, one of my goals was to switch from {Kafka} to {RabbitMQ}.

NOTE: To start this lab, in order to have sufficient memory for executing the following steps, I had to quit Docker.

== Prerequisites

. <<lab1,Lab 1>> executed.
. {VirtualBox} installed (PCF Dev uses it).
. An account on {uri-pivotal-cloud-foundry}[Pivotal Web Services] (do download PCF Dev binary).

== Step 1 - Download and install PCF Dev

First of all, after I downloaded the https://network.pivotal.io/products/pcfdev[PCF Dev binary], I typed the following commands to install it:

----
$ unzip pcfdev-v0.30.0+PCF1.11.0-osx.zip
Archive:  pcfdev-v0.30.0+PCF1.11.0-osx.zip
  inflating: pcfdev-v0.30.0+PCF1.11.0-osx

$ ./pcfdev-v0.30.0+PCF1.11.0-osx
Plugin successfully upgraded. Current version: 0.30.0. For more info run: cf dev help
----

To check the current version, I typed:

----
$ cf dev version
PCF Dev version 0.30.0 (CLI: 850ae45, OVA: 0.549.0)
----

== Step 2 - Start PCF Dev

I started PCF Dev with more memory than default (4096 MB makes the services very hard to work):

----
$ cf dev start -m 8192
Downloading VM...
Progress: |====================>| 100%
VM downloaded.
Allocating 8192 MB out of 16384 MB total system memory (9217 MB free).
Importing VM...
Starting VM...
Provisioning VM...
Waiting for services to start...
7 out of 58 running
7 out of 58 running
7 out of 58 running
7 out of 58 running
40 out of 58 running
56 out of 58 running
58 out of 58 running
 _______  _______  _______    ______   _______  __   __
|       ||       ||       |  |      | |       ||  | |  |
|    _  ||       ||    ___|  |  _    ||    ___||  |_|  |
|   |_| ||       ||   |___   | | |   ||   |___ |       |
|    ___||      _||    ___|  | |_|   ||    ___||       |
|   |    |     |_ |   |      |       ||   |___  |     |
|___|    |_______||___|      |______| |_______|  |___|
is now running.
To begin using PCF Dev, please run:
   cf login -a https://api.local.pcfdev.io --skip-ssl-validation
Apps Manager URL: https://apps.local.pcfdev.io
Admin user => Email: admin / Password: admin
Regular user => Email: user / Password: pass
----

[NOTE]
====
The command above takes some time (around 15 minutes on my MacBook Pro) because it has to download the VM (in https://en.wikipedia.org/wiki/Open_Virtualization_Format[OVA format]) and starts a bunch of services (the most time-consuming part of the procedure).
After it finishes I verified the directory structure where the machine was installed:

----
$ tree ~/.pcfdev/
/Users/pj/.pcfdev/
|-- ova
|   `-- pcfdev-v0.549.0.ova
|-- token
`-- vms
    |-- key.pem
    |-- pcfdev-v0.549.0
    |   |-- Logs
    |   |   |-- VBox.log
    |   |   `-- VBox.log.1
    |   |-- pcfdev-v0.549.0-disk1.vmdk
    |   |-- pcfdev-v0.549.0.vbox
    |   `-- pcfdev-v0.549.0.vbox-prev
    `-- vm_config

4 directories, 9 files
----
====

== Step 3 - Login into PCF Dev

When PCF Dev was started, I made my login:

----
$ cf login -a https://api.local.pcfdev.io --skip-ssl-validation -u admin -p admin -o pcfdev-org
API endpoint: https://api.local.pcfdev.io
Authenticating...
OK

Targeted org pcfdev-org

Targeted space pcfdev-space



API endpoint:   https://api.local.pcfdev.io (API version: 2.82.0)
User:           admin
Org:            pcfdev-org
Space:          pcfdev-space
----

== Step 4 - Create services

After that, I created three services:

{MySQL} service:

----
$ cf create-service p-mysql 512mb df-mysql
Creating service instance df-mysql in org pcfdev-org / space pcfdev-space as admin...
OK
----

{RabbitMQ} service:

----
$ cf create-service p-rabbitmq standard df-rabbitmq
Creating service instance df-rabbitmq in org pcfdev-org / space pcfdev-space as admin...
OK
----

{Redis} service:

----
$ cf create-service p-redis shared-vm df-redis
Creating service instance df-redis in org pcfdev-org / space pcfdev-space as admin...
OK
----

== Step 5 - Download some jars

----
$ cd ~/labs/spring-cloud-dataflow
$ wget -c http://repo.spring.io/release/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/1.4.0.RELEASE/spring-cloud-dataflow-server-cloudfoundry-1.4.0.RELEASE.jar
----

== Step 6 - Create a manifest.yml file

I copied `tutorial/file/manifest.yml` file to the current dir:

----
$ cp ../tutorial/files/manifest.yml .
----

NOTE: The `tutorial` dir was created on <<lab1-step9,lab 1, step 9>>.

The content of this file is presented below:

[source,yaml]
----
include::{projectdir}/files/manifest.yml[]
----

NOTE: The session {uri-spring-cloud-dataflow-server-cloudfoundry-doc}/#getting-started-cloudfoundry-deploying-using-manifest["Deploying using a Manifest"] of the document {uri-spring-cloud-dataflow-server-cloudfoundry-doc}["Spring Cloud Data Flow Server for Cloud Foundry"] details this file.

== Step 7 - Push spring-cloud-dataflow-server-cloudfoundry to PCF Dev

----
$ cf push
Pushing from manifest to org pcfdev-org / space pcfdev-space as admin...
Using manifest file /Users/pj/labs/spring-cloud-dataflow/manifest.yml
Getting app info...
Creating app with these attributes...
+ name:         dataflow-server
  path:         /Users/pj/labs/spring-cloud-dataflow/spring-cloud-dataflow-server-cloudfoundry-1.4.0.RELEASE.jar
+ buildpack:    java_buildpack
+ disk quota:   2G
+ memory:       2G
  services:
+   df-mysql
+   df-redis
  env:
+   MAVEN_REMOTE_REPOSITORIES_REPO1_URL
+   SPRING_CLOUD_DATAFLOW_FEATURES_EXPERIMENTAL_TASKSENABLED
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_BUILDPACK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_DISK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_INSTANCES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_MEMORY
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_BUILDPACK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_DISK
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_ENABLE_RANDOM_APP_NAME_PREFIX
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_INSTANCES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_MEMORY
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL
+   SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME
  routes:
+   dataflow-server.local.pcfdev.io

Creating app dataflow-server...
Mapping routes...
Binding services...
Comparing local files to remote cache...
Packaging files to upload...
Uploading files...
 71.59 MiB / 71.59 MiB [==========================================================================================================================] 100.00% 2s

Waiting for API to complete processing files...

Staging app and tracing logs...
   Downloading java_buildpack...
   Downloaded java_buildpack (244.5M)
   Creating container
   Successfully created container
   Downloading app package...
   Downloaded app package (71.6M)
   Staging...
   -----> Java Buildpack Version: v3.13 (offline) | https://github.com/cloudfoundry/java-buildpack.git#03b493f
   -----> Downloading Open Jdk JRE 1.8.0_121 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_121.tar.gz (found in cache)
          Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.0s)
   -----> Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (found in cache)
          Memory Settings: -Xmx1363148K -XX:MaxMetaspaceSize=209715K -Xms1363148K -XX:MetaspaceSize=209715K -Xss699K
   -----> Downloading Container Certificate Trust Store 2.0.0_RELEASE from https://java-buildpack.cloudfoundry.org/container-certificate-trust-store/container-certificate-trust-store-2.0.0_RELEASE.jar (found in cache)
          Adding certificates to .java-buildpack/container_certificate_trust_store/truststore.jks (0.4s)
   -----> Downloading Spring Auto Reconfiguration 1.10.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.10.0_RELEASE.jar (found in cache)
   Exit status 0
   Staging complete
   Uploading droplet, build artifacts cache...
   Uploading build artifacts cache...
   Uploading droplet...
   Uploaded build artifacts cache (108B)
   Uploaded droplet (116.9M)
   Uploading complete
   Destroying container
   Successfully destroyed container

Waiting for app to start...

name:              dataflow-server
requested state:   started
instances:         1/1
usage:             2G x 1 instances
routes:            dataflow-server.local.pcfdev.io
last uploaded:     Tue 01 May 19:48:22 WEST 2018
stack:             cflinuxfs2
buildpack:         java_buildpack
start command:     CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE
                   -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100%
                   -stackThreads=300 -totMemory=$MEMORY_LIMIT) && JAVA_OPTS="-Djava.io.tmpdir=$TMPDIR
                   -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY
                   -Djavax.net.ssl.trustStore=$PWD/.java-buildpack/container_certificate_trust_store/truststore.jks
                   -Djavax.net.ssl.trustStorePassword=java-buildpack-trust-store-password" && SERVER_PORT=$PORT eval exec
                   $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher

     state     since                  cpu      memory         disk         details
#0   running   2018-05-01T18:49:23Z   220.0%   782.6M of 2G   206M of 2G   
----

== Step 8 - Access the dashboard

After deployed, I pointed my browser to to following URL: http://dataflow-server.local.pcfdev.io/dashboard/.

I noticed that the dashboard gives me a information that it has no applications installed.

== Step 9 - Use the shell

In my <<lab1,previous lab>>, I used the UI interface (through the browser) to create a stream.
This time, however, I created the stream by using the command line.
These were my steps:

----
$ wget -c http://repo.spring.io/release/org/springframework/cloud/spring-cloud-dataflow-shell/1.4.0.RELEASE/spring-cloud-dataflow-shell-1.4.0.RELEASE.jar
----

----
$ java -jar spring-cloud-dataflow-shell-1.4.0.RELEASE.jar 
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

1.4.0.RELEASE

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:>
----

I configured the shell to access my local PCF Dev instance:

----
server-unknown:>dataflow config server http://dataflow-server.local.pcfdev.io
Shell mode: classic, Server mode: classic
dataflow:>
----

I imported the applications:

----
dataflow:>app import --uri http://bit.ly/Celsius-SR1-stream-applications-rabbit-maven
Successfully registered 65 applications from [source.sftp, source.mqtt.metadata, sink.mqtt.metadata, source.file.metadata, processor.tcp-client, source.s3.metadata, source.jms, source.ftp, processor.transform.metadata, source.time, sink.mqtt, sink.s3.metadata, processor.scriptable-transform, sink.log, source.load-generator, processor.transform, source.syslog, sink.websocket.metadata, sink.task-launcher-local.metadata, source.loggregator.metadata, source.s3, source.load-generator.metadata, processor.pmml.metadata, source.loggregator, source.tcp.metadata, processor.httpclient.metadata, sink.file.metadata, source.triggertask, source.twitterstream, source.gemfire-cq.metadata, processor.aggregator.metadata, source.mongodb, source.time.metadata, source.gemfire-cq, sink.counter.metadata, source.http, sink.tcp.metadata, sink.pgcopy.metadata, source.rabbit, sink.task-launcher-yarn, source.jms.metadata, sink.gemfire.metadata, sink.cassandra.metadata, processor.tcp-client.metadata, processor.header-enricher, sink.throughput, sink.task-launcher-local, processor.python-http, sink.aggregate-counter.metadata, sink.mongodb, processor.twitter-sentiment, sink.log.metadata, processor.splitter, sink.hdfs-dataset, source.tcp, processor.python-jython.metadata, source.trigger, source.mongodb.metadata, processor.bridge, source.http.metadata, source.rabbit.metadata, sink.ftp, sink.jdbc, source.jdbc.metadata, source.mqtt, processor.pmml, sink.aggregate-counter, sink.rabbit.metadata, processor.python-jython, sink.router.metadata, sink.cassandra, processor.filter.metadata, source.tcp-client.metadata, processor.header-enricher.metadata, processor.groovy-transform, source.ftp.metadata, sink.router, sink.redis-pubsub, source.tcp-client, processor.httpclient, sink.file, sink.websocket, source.syslog.metadata, sink.s3, sink.counter, sink.rabbit, processor.filter, source.trigger.metadata, source.mail.metadata, sink.gpfdist.metadata, sink.pgcopy, processor.python-http.metadata, sink.jdbc.metadata, sink.gpfdist, sink.ftp.metadata, processor.splitter.metadata, sink.sftp, sink.field-value-counter, processor.groovy-filter.metadata, processor.twitter-sentiment.metadata, source.triggertask.metadata, sink.hdfs, processor.groovy-filter, sink.redis-pubsub.metadata, source.sftp.metadata, processor.bridge.metadata, sink.field-value-counter.metadata, processor.groovy-transform.metadata, processor.aggregator, sink.sftp.metadata, processor.tensorflow.metadata, sink.throughput.metadata, sink.hdfs-dataset.metadata, sink.tcp, source.mail, sink.task-launcher-cloudfoundry.metadata, source.gemfire.metadata, processor.tensorflow, source.jdbc, sink.task-launcher-yarn.metadata, sink.gemfire, source.gemfire, source.twitterstream.metadata, sink.hdfs.metadata, processor.tasklaunchrequest-transform, sink.task-launcher-cloudfoundry, source.file, sink.mongodb.metadata, processor.tasklaunchrequest-transform.metadata, processor.scriptable-transform.metadata]
----

I refresh my browser and noticed that, now, I had many applications installed (65 according to the last log).

I created the stream:

----
dataflow:>stream create --name httptest --definition "http | log" --deploy
Created new stream 'httptest'
Deployment request has been sent
----

I noticed that the stream was deployed:

----
dataflow:>stream list
╔═══════════╤═════════════════╤═════════════════════════════════════════╗
║Stream Name│Stream Definition│                 Status                  ║
╠═══════════╪═════════════════╪═════════════════════════════════════════╣
║httptest   │http | log       │The stream has been successfully deployed║
╚═══════════╧═════════════════╧═════════════════════════════════════════╝
----

I started another shell (leaving the current shell opened) and then I typed the following command:

----
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                            requested state   instances   memory   disk   urls
dataflow-server                 started           1/1         2G       2G     dataflow-server.local.pcfdev.io
dataflow-server-httptest-log    started           1/1         512M     512M   dataflow-server-httptest-log.local.pcfdev.io
dataflow-server-httptest-http   started           1/1         512M     512M   dataflow-server-httptest-http.local.pcfdev.io
----

In order to view the log output for `dataflow-server-httptest-log`, I typed:

-----
$ cf logs dataflow-server-httptest-log
Retrieving logs for app dataflow-httptest-log in org pcfdev-org / space pcfdev-space as admin...

-----

Back to the first shell (running `spring-cloud-dataflow-shell`), I typed:

----
dataflow:>http post --target http://dataflow-server-httptest-http.local.pcfdev.io --data "hello world"
> POST (text/plain;Charset=UTF-8) http://dataflow-httptest-http.local.pcfdev.io hello world
> 202 ACCEPTED

dataflow:>http post --target http://dataflow-server-httptest-http.local.pcfdev.io --data "paulo jeronimo"
> POST (text/plain) http://dataflow-server-httptest-http.local.pcfdev.io paulo jeronimo
> 202 ACCEPTED
----

So, this lines appears in the output of the command `cf logs dataflow-server-httptest-log`:

----
   2018-04-29T14:15:04.21+0100 [APP/PROC/WEB/0] OUT 2018-04-29 13:15:04.215  INFO 6 --- [http.httptest-1] log.sink                                 : hello world
   2018-04-29T14:17:22.83+0100 [APP/PROC/WEB/0] OUT 2018-04-29 13:17:22.832  INFO 6 --- [http.httptest-1] log.sink                                 : paulo jeronimo
----

My last command was delete the created stream:

----
dataflow:>stream destroy --name httptest
Destroyed stream 'httptest'
dataflow:>exit
----

== Step 10 - Stop PCF Dev

----
cf dev stop
----

== Step 11 - (Optional) Destroy the PCF Dev environment

----
cf dev destroy
----

[[lab2-conclusions]]
== Conclusions

Positive points: ::
* The PCF Dev also makes the deployment of SCDF components very simple.
The `cf` command makes easy, in a development environment, to push new sources, sinks, or processors to the servers.
* The shell interface provided by <<spring-cloud-dataflow-shell>> is extremely useful to automate things.
This kind of tool is totally required in a DevOps world.
* Based on this lab, I can deduce that in a large corporation with {uri-pivotal-cloud-foundry}[Pivotal Cloud Foundry] installed on its servers, SCDF is definitely prepared and integrated with this kind of infrastructure.
Negative points: ::
* The time to start PCF Dev on my machine was very slow compared to the time to up Docker containers with Docker compose.
So, I would recommend the use of Docker in a developer environment instead of PCF Dev.

== References

* {oA}
* {uri-spring-cloud-dataflow-samples-docs}[Spring Cloud Data Flow Samples Documentation]
* https://pivotal.io/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/install-pcf-dev[Install PCF Dev CLI].
